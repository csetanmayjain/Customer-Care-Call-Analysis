{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9313275,
          "sourceType": "datasetVersion",
          "datasetId": 5640482
        },
        {
          "sourceType": "datasetVersion",
          "sourceId": 9391491,
          "datasetId": 5699135
        }
      ],
      "dockerImageVersionId": 30762,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/csetanmayjain/Customer-Care-Call-Analysis/blob/main/Customer_Care_Call_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Input:** Expected Audio: Stereo Type\n",
        "\n",
        "**Output:** Summarization of the audio with the following insights:\n",
        "\n",
        "        1. Problem Resolution\n",
        "        2. Important Keyword Detection\n",
        "        3. Agent Behavior Analysis\n",
        "        4. Product Enhancement Opportunities\n",
        "        5. New Product Features\n",
        "\n",
        "**Approach:**\n",
        "To achieve the output, we used the following tools and frameworks:\n",
        "\n",
        "  1. Silero VAD: For audio chunking and voice activity detection (VAD)\n",
        "  2. NVIDIA NeMo: For performing Automatic Speech Recognition (ASR)\n",
        "  3. LLaMA 3: For analyzing and extracting insights from the transcription using a large language model (LLM)\n"
      ],
      "metadata": {
        "id": "TP4gKJRqDGXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Install vad dependencies\n",
        "!pip install --quiet pydub\n",
        "!pip install --quiet silero-vad\n",
        "\n",
        "## Install ASR dependencies\n",
        "!pip install --quiet wget\n",
        "!apt-get install -y sox libsndfile1 ffmpeg\n",
        "!pip install --quiet text-unidecode\n",
        "\n",
        "## Install NeMo\n",
        "BRANCH = 'main'\n",
        "!python -m pip --quiet install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[asr]\n",
        "\n",
        "## Install LLM dependencies\n",
        "!pip install --quiet groq\n",
        "!pip install --quiet gradio"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:43:43.834865Z",
          "iopub.execute_input": "2024-09-14T09:43:43.835151Z",
          "iopub.status.idle": "2024-09-14T09:46:31.590902Z",
          "shell.execute_reply.started": "2024-09-14T09:43:43.83512Z",
          "shell.execute_reply": "2024-09-14T09:46:31.58975Z"
        },
        "trusted": true,
        "id": "zUmC1T3B9khu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import vad dependencies\n",
        "from silero_vad import load_silero_vad, read_audio, get_speech_timestamps\n",
        "from pydub import AudioSegment\n",
        "\n",
        "#import asr dependencies\n",
        "import nemo.collections.asr as nemo_asr\n",
        "from nemo.utils import logging\n",
        "logging.setLevel(logging.CRITICAL)\n",
        "\n",
        "#import llm dependencies\n",
        "from groq import Groq\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from huggingface_hub import login"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:31.592331Z",
          "iopub.execute_input": "2024-09-14T09:46:31.592657Z",
          "iopub.status.idle": "2024-09-14T09:46:45.923743Z",
          "shell.execute_reply.started": "2024-09-14T09:46:31.592622Z",
          "shell.execute_reply": "2024-09-14T09:46:45.922988Z"
        },
        "trusted": true,
        "id": "aP7NtC1s9khv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_models(asr_model_path):\n",
        "\n",
        "    login(token=os.environ['hf_token'])\n",
        "\n",
        "    global vad_model, asr_model, llm_model\n",
        "\n",
        "    if vad_model == None:\n",
        "        vad_model = load_silero_vad()\n",
        "\n",
        "    if asr_model == None:\n",
        "        asr_model = nemo_asr.models.EncDecCTCModelBPE.restore_from(asr_model_path)\n",
        "\n",
        "    return vad_model, asr_model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:45.932699Z",
          "iopub.execute_input": "2024-09-14T09:46:45.933082Z",
          "iopub.status.idle": "2024-09-14T09:46:45.950239Z",
          "shell.execute_reply.started": "2024-09-14T09:46:45.933038Z",
          "shell.execute_reply": "2024-09-14T09:46:45.949442Z"
        },
        "trusted": true,
        "id": "guOUMLxM9khw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_stereo_to_mono(input_file, output_left, output_right):\n",
        "\n",
        "    # Load the stereo audio file\n",
        "    audio = AudioSegment.from_file(input_file)\n",
        "\n",
        "    # Ensure the audio is stereo\n",
        "    if audio.channels != 2:\n",
        "        print(\"The input audio is not stereo.\")\n",
        "        return\n",
        "\n",
        "    # Split into left and right channels\n",
        "    left_channel = audio.split_to_mono()[0]\n",
        "    right_channel = audio.split_to_mono()[1]\n",
        "\n",
        "    # Set sample rate to 16000 Hz and bit depth to 16 bits\n",
        "    left_channel = left_channel.set_frame_rate(16000).set_sample_width(2)\n",
        "    right_channel = right_channel.set_frame_rate(16000).set_sample_width(2)\n",
        "\n",
        "    # Export the left and right channels as separate mono files\n",
        "    left_channel.export(output_left, format=\"wav\")\n",
        "    right_channel.export(output_right, format=\"wav\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:45.951196Z",
          "iopub.execute_input": "2024-09-14T09:46:45.951467Z",
          "iopub.status.idle": "2024-09-14T09:46:45.961022Z",
          "shell.execute_reply.started": "2024-09-14T09:46:45.951437Z",
          "shell.execute_reply": "2024-09-14T09:46:45.960238Z"
        },
        "trusted": true,
        "id": "DkTKap9N9khx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_chunks_timestamps(audio_file_path):\n",
        "    split_stereo_to_mono(audio_file_path, \"left_channel.wav\", \"right_channel.wav\")\n",
        "\n",
        "    speaker0 = read_audio('left_channel.wav') # backend (sox, soundfile, or ffmpeg) required!\n",
        "    speaker0_speech_timestamps = get_speech_timestamps(speaker0, vad_model)\n",
        "\n",
        "    speaker1 = read_audio('right_channel.wav') # backend (sox, soundfile, or ffmpeg) required!\n",
        "    speaker1_speech_timestamps = get_speech_timestamps(speaker1, vad_model)\n",
        "\n",
        "    # Combine both lists with an additional key to store their origin\n",
        "    combined = [{'start': item['start'], 'end': item['end'], 'origin': 'speaker0'} for item in speaker0_speech_timestamps] + \\\n",
        "               [{'start': item['start'], 'end': item['end'], 'origin': 'speaker1'} for item in speaker1_speech_timestamps]\n",
        "\n",
        "    # Sort the combined list based on the start time\n",
        "    sorted_combined = sorted(combined, key=lambda x: x['start'])\n",
        "\n",
        "    audio_chunks = []\n",
        "    for sorted_combined_i in sorted_combined:\n",
        "        start = sorted_combined_i['start']\n",
        "        end = sorted_combined_i['end']\n",
        "        origin = sorted_combined_i['origin']\n",
        "\n",
        "        if origin == \"speaker0\":\n",
        "            audio_chunks.append(speaker0[start:end])\n",
        "\n",
        "        if origin == \"speaker1\":\n",
        "            audio_chunks.append(speaker1[start:end])\n",
        "\n",
        "    os.remove(\"left_channel.wav\")\n",
        "    os.remove(\"right_channel.wav\")\n",
        "\n",
        "    return sorted_combined, audio_chunks"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:45.962066Z",
          "iopub.execute_input": "2024-09-14T09:46:45.962363Z",
          "iopub.status.idle": "2024-09-14T09:46:45.974887Z",
          "shell.execute_reply.started": "2024-09-14T09:46:45.962333Z",
          "shell.execute_reply": "2024-09-14T09:46:45.974089Z"
        },
        "trusted": true,
        "id": "26hCNAyy9khz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ASR"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:45.976317Z",
          "iopub.execute_input": "2024-09-14T09:46:45.976707Z",
          "iopub.status.idle": "2024-09-14T09:46:45.987011Z",
          "shell.execute_reply.started": "2024-09-14T09:46:45.976648Z",
          "shell.execute_reply": "2024-09-14T09:46:45.986265Z"
        },
        "trusted": true,
        "id": "_mKChHyU9kh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_asr(audio_chunks, timestamping):\n",
        "\n",
        "    # Perform inference\n",
        "    transcriptions = asr_model.transcribe(audio=audio_chunks, batch_size=8)\n",
        "\n",
        "    text = \"\"\n",
        "    for i, transcription in enumerate(transcriptions):\n",
        "    #     print(f\"{timestamping[i]['origin']}: {transcription}\")\n",
        "\n",
        "        text += timestamping[i]['origin'] + \": \" + transcription\n",
        "        text += \"\\n\"\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:45.988052Z",
          "iopub.execute_input": "2024-09-14T09:46:45.988328Z",
          "iopub.status.idle": "2024-09-14T09:46:45.996315Z",
          "shell.execute_reply.started": "2024-09-14T09:46:45.988298Z",
          "shell.execute_reply": "2024-09-14T09:46:45.995487Z"
        },
        "trusted": true,
        "id": "si71aeSF9kh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:45.998933Z",
          "iopub.execute_input": "2024-09-14T09:46:45.999263Z",
          "iopub.status.idle": "2024-09-14T09:46:46.005928Z",
          "shell.execute_reply.started": "2024-09-14T09:46:45.999232Z",
          "shell.execute_reply": "2024-09-14T09:46:46.005137Z"
        },
        "trusted": true,
        "id": "tNLTdOTd9kh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_analysis(text):\n",
        "\n",
        "    output = \"\"\n",
        "    client = Groq()\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"llama3-8b-8192\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Give the output in english only.\\nWrite a summary of the chunk of text that includes the main points and any important details.\\nAdditionally extract the following six insights from the conversation:\\n1. Sentiment Analysis: What is the customer's sentiment in one word from the following Satisfaction, Anger, Frustration, Resolution, Escalation.\\n2. Problem Resolution: Was the issue resolved? (In-Progress/ Resolved/ Unresolved/ Unclear)\\n3. Keyword Detection: List important keyword or phrase spoken if any\\n4. Agent Behavior: Evaluate the agent's performance and professionalism up to 2 words only.\\n5. Enhancement Opportunities: Suggest ways to improve service or operations.\\n6. New Product Features: Identify any new feature requests or suggestions that can be incorporated in the existing product.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": text\n",
        "            }\n",
        "        ],\n",
        "        temperature=0.6,\n",
        "        max_tokens=1024,\n",
        "        top_p=1,\n",
        "        stream=True,\n",
        "        stop=None,\n",
        "    )\n",
        "\n",
        "    output = \"\"\n",
        "    for chunk in completion:\n",
        "        output += chunk.choices[0].delta.content or \"\"\n",
        "\n",
        "    return output\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:46.007136Z",
          "iopub.execute_input": "2024-09-14T09:46:46.00743Z",
          "iopub.status.idle": "2024-09-14T09:46:46.0158Z",
          "shell.execute_reply.started": "2024-09-14T09:46:46.0074Z",
          "shell.execute_reply": "2024-09-14T09:46:46.014892Z"
        },
        "trusted": true,
        "id": "pXC9lsGf9kh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Driver Function"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:46.016974Z",
          "iopub.execute_input": "2024-09-14T09:46:46.017328Z",
          "iopub.status.idle": "2024-09-14T09:46:46.027803Z",
          "shell.execute_reply.started": "2024-09-14T09:46:46.017287Z",
          "shell.execute_reply": "2024-09-14T09:46:46.026958Z"
        },
        "trusted": true,
        "id": "8M5KnAMB9kh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global vad_model, asr_model\n",
        "vad_model = None\n",
        "asr_model = None"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:46.028807Z",
          "iopub.execute_input": "2024-09-14T09:46:46.029055Z",
          "iopub.status.idle": "2024-09-14T09:46:46.037017Z",
          "shell.execute_reply.started": "2024-09-14T09:46:46.029027Z",
          "shell.execute_reply": "2024-09-14T09:46:46.036223Z"
        },
        "trusted": true,
        "id": "eFD83tti9kh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def driver_code(input_audio_path):\n",
        "\n",
        "    print(\"Getting Speaker Timestamp\")\n",
        "    timestamping, audio_chunks = get_chunks_timestamps(input_audio_path)\n",
        "\n",
        "    print(\"Getting Transcription\")\n",
        "    text = get_asr(audio_chunks, timestamping)\n",
        "\n",
        "    print(\"Getting Analysis\")\n",
        "    analysis = get_analysis(text)\n",
        "\n",
        "    return analysis"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:46:52.089062Z",
          "iopub.execute_input": "2024-09-14T09:46:52.089395Z",
          "iopub.status.idle": "2024-09-14T09:46:52.095384Z",
          "shell.execute_reply.started": "2024-09-14T09:46:52.089358Z",
          "shell.execute_reply": "2024-09-14T09:46:52.094235Z"
        },
        "trusted": true,
        "id": "4hh0xNIE9kh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up environment variables for Hugging Face token and GROQ API key\n",
        "os.environ['hf_token'] = \"\"  # Insert your Hugging Face token here\n",
        "os.environ['GROQ_API_KEY'] = \"\"  # Insert your GROQ API key here\n",
        "\n",
        "# Define the path for the Nemo ASR model to be used\n",
        "asr_model_path = \"\"  # Specify the path where your Nemo ASR model is stored\n",
        "\n",
        "# Check if the VAD (Voice Activity Detection) and ASR models are already loaded\n",
        "if vad_model == None or asr_model == None:\n",
        "    # If either VAD or ASR models are not loaded, load them from the specified path\n",
        "    vad_model, asr_model = load_models(asr_model_path)\n",
        "\n",
        "# Provide the path for the input audio file that needs to be processed\n",
        "input_audio_path = \"\"  # Specify the path to the input audio file\n",
        "\n",
        "# Perform analysis on the audio using the loaded models\n",
        "analysis = driver_code(input_audio_path)  # Call the driver function to analyze the audio\n"
      ],
      "metadata": {
        "id": "AyyXvX8EEib_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(analysis)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-09-14T09:47:08.595316Z",
          "iopub.execute_input": "2024-09-14T09:47:08.595624Z",
          "iopub.status.idle": "2024-09-14T09:47:08.600267Z",
          "shell.execute_reply.started": "2024-09-14T09:47:08.595591Z",
          "shell.execute_reply": "2024-09-14T09:47:08.599404Z"
        },
        "trusted": true,
        "id": "A4eaKziO9kh6",
        "outputId": "7a5dc81f-ee4e-4d75-ddf4-864323f45e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "**Summary:**\n\nThe customer is upset about the delayed delivery of their order, which was supposed to be delivered two days ago. The agent apologizes and confirms that the order is being processed. The customer asks to schedule a delivery time and the agent agrees to send a delivery person between 10:00 am to 1:00 pm the next day. The customer requests that the delivery person calls them before arriving and that they will be available at home after 6:00 pm. The agent confirms the details and assures the customer that the delivery will be made.\n\n**Insights:**\n\n1. **Sentiment Analysis:** Frustration\n2. **Problem Resolution:** Resolved\n3. **Keyword Detection:** Delivery, Order Number, Schedule, Security\n4. **Agent Behavior:** Professional and courteous\n5. **Enhancement Opportunities:** Improve communication with customers about delivery schedules and provide a clear timeline for delivery.\n6. **New Product Features:** None mentioned\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}